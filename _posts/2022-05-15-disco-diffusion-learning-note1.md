---
layout: post
comments: true
title: "Disco Diffusion v5.2 Learning Note 1"
date: 2022-05-15 02:00:00
tags: generative_models
---


> The background of Disco Diffusion, including the brief introduction of CLIP and Diffusion model. This article is written for those researchers and engineers who seek to understand the underlying mechanism of the Disco Diffusion. If you just want to run the Disco Diffusion as a black-box system, you can skip this article.

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}


## What is Disco Diffusion

![My Logo]({{ '/assets/images/20220510_disco_diffusion1.png' | relative_url }})
{: style="width: 100%;" class="center"}
*Fig. 1: An image generated by Disco Diffusion v5.2 \[[1](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb)\] with the default prompt "A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood by greg rutkowski and thomas kinkade, Trending on artstation."
{:.image-caption}

Disco Diffusion (DD) \[[1](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb), [2](https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit)\] is an open-source project that generates high-quality images from the given text description, which is also known as *prompt*. The technique used by DD is called CLIP-guided Diffusion, where the CLIP \[[3](https://openai.com/blog/clip/)\] is an multi-modal pretrained model that connecting text and images, and diffusion model [4,5,6,7] is a kind of generative model that iteratively update the image from Gaussian noise to a cleaner and more detailed version of the original one (Fig. 2). Combining the above two techniques, DD is then created by [Somnai](https://twitter.com/Somnai_dreams), augmented by [Gandamu](https://twitter.com/gandamu_ml), and building on the work of [RiversHaveWings](https://twitter.com/RiversHaveWings), [nshepperd](https://twitter.com/nshepperd1), and many other collaborators.  

A demo of Disco Diffusion can also be freely obtained, modified, and executed on [Google Colab Notebook](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb).

A detailed introduction of Disco Diffusion can be found at [URL](https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit).

![My Logo]({{ '/assets/images/20220510_disco_diffusion2.png' | relative_url }})
{: style="width: 100%;" class="center"}
*Fig. 2: an example of intermediate images generated at steps 1, 50, 100, 150, and 200 of the diffusion process using the prompt: "A girl and a boy meet each other in a forest under the stars from a Hayao Miyazaki animation, Trending on twitter."
{:.image-caption}

## What is CLIP

CLIP is a multimodal neural network proposed by OpenAI \[[3](https://openai.com/blog/clip/)\], which learns correlation between visual concepts and language descriptions. It not just equips the visual model with a powerful "zero-shot" capabilities by converting the conventional classification task into a vision-and-language matching problem, *e.g.*, as shown in Fig. 3(2), we can generate different text descriptions for each category, which allows the zero-shot prediction on CLIP model, but also empowers the generative models to create images from text descriptions, *e.g.*, [Disco Diffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb), [VQGAN+CLIP](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb).


![My Logo]({{ '/assets/images/20220510_clip.png' | relative_url }})
{: style="width: 100%;" class="center"}
*Fig. 3: 1). how does the CLIP model get pre-trained; 2) how to convert the classification task into a vision-and-language matching problem
{:.image-caption}


### The summary of CLIP

The basic idea behind CLIP is very simple and intuitive: if we have lots of (image, descriptive text) pairs, we can consider each pair as an positive training sample and use others as negative samples. As we can see from Fig.3 (1), after extracting text features $$\{T_i\}$$ and image features $$\{I_i\}$$, all the diagonal elements at the matching matrix $$(T_i, I_i)$$ are positive matching results and others $$(T_i, I_j)$$, $$i \neq j$$ are negative matching results. Therefore, both the text encoder and the image encoder can be training by massive image and captioning pairs on the internet. The following pseudocode can be used to better understand how models are learned by CLIP.


### The pseudocode for the core of an implementation of CLIP

```bash
# image_encoder - ResNet or Vision Transformer 
# text_encoder - CBOW or Text Transformer 
# I[n, h, w, c] - minibatch of aligned images 
# T[n, l] - minibatch of aligned texts 
# W_i[d_i, d_e] - learned proj of image to embed 
# W_t[d_t, d_e] - learned proj of text to embed 
# t - learned temperature parameter 

# extract feature representations of each modality 
I_f = image_encoder(I) #[n, d_i] 
T_f = text_encoder(T) #[n, d_t] 

# joint multimodal embedding [n, d_e] 
I_e = l2_normalize(np.dot(I_f, W_i), axis=1) 
T_e = l2_normalize(np.dot(T_f, W_t), axis=1) 

# scaled pairwise cosine similarities [n, n] 
logits = np.dot(I_e, T_e.T) * np.exp(t) 

# symmetric loss function 
labels = np.arange(n) 
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1) 
loss = (loss_i + loss_t)/2
```

### The application of CLIP

1. The CLIP is originally designed to solve the **zero-shot** transfer learning problem for downstream tasks. By placing the name of a category into a sentence, *e.g.*, snow leopard -> A photo of a snow leopard. The visual model could be equipped with the zero-shot ability, predicting a novel category of object without fine-tuning. 

2. Due to the powerful vision-and-language matching ability, CLIP model further makes text-driven image generation possible. Plenty of applications, like [Disco Diffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb), [VQGAN+CLIP](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb), and [DALL·E 2](https://openai.com/dall-e-2/) are built on top of the CLIP model, which provides a matching score between the generated image and the input text and thus guide the generation process to optimize such a score.


## What is Diffusion Model


## How Disco Diffusion Works


## References

[1] Disco Diffusion v5.2 Google Colab Notebook [URL](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb)

[2] Zippy's Disco Diffusion Cheatsheet v0.3 [URL](https://docs.google.com/document/d/1l8s7uS2dGqjztYSjPpzlmXLjl5PM3IGkRWI3IiCuK7g/edit)

[3] CLIP: Connecting Text and Images [URL](https://openai.com/blog/clip/)

[4] Jascha Sohl-Dickstein et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ICML 2015. 

[5] Alex Nichol & Prafulla Dhariwal. “ Improved denoising diffusion probabilistic models” arxiv Preprint arxiv:2102.09672 (2021).

[6] Prafula Dhariwal & Alex Nichol. “Diffusion Models Beat GANs on Image Synthesis." arxiv Preprint arxiv:2105.05233 (2021). 

[7] What are Diffusion Models? Lil'Log [URL](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

## Emphasis

*This text will be italic*
_This will also be italic_

**This text will be bold**
__This will also be bold__

_You **can** combine them_



## Lists

Unordered
- Item 1
- Item 2
  - Item 2a
  - Item 2b


Ordered
1. Item 1
2. Item 2
3. Item 3
   1. Item 3a
   2. Item 3b


## Images


![My Logo]({{ '/assets/logos/logo_home.png' | relative_url }})
{: style="width: 50%;" class="center"}
*Fig. 1: This Logo is created by Troll (https://dribbble.com/shots/4668586-Red-Panda). All rights reserved by Him/Her.*
{:.image-caption}


## Links
[GitHub](http://github.com)


## Blockquotes
As Kanye West said:

> We're living the future so
> the present is our past.


## Inline code
To print some text with python, you should use the `print()` function.
```
print("Hello world!")
```


## Example of Mathematic Fomulars

$$
\mathcal{L}_u^\Pi = \sum_{\mathbf{x} \in \mathcal{D}} \text{MSE}(f_\theta(\mathbf{x}), f'_\theta(\mathbf{x}))
$$

$$n$$ feed-forward networks as experts $$\{E_i\}^n_{i=1}$$

$$p(x)$$, $$p(y \mid x)$$, $$\frac{p(x)}{p(y)}$$, $$p(y \mid x)=\frac{p(x \mid y)p(y)}{p(x)}$$

