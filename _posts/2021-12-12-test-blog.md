---
layout: post
comments: true
title: "Test: Hello World"
date: 2021-12-12 20:00:00
tags: test
---


> The abstract of the article.

<!--more-->


When facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed.
1. *Pre-training + fine-tuning*: Pre-train a powerful task-agnostic model on a large unsupervised data corpus, e.g. [pre-training LMs]({{ site.baseurl }}{% post_url 2019-01-31-generalized-language-models %}) on free text, or pre-training vision models on unlabelled images via [self-supervised learning]({{ site.baseurl }}{% post_url 2019-11-10-self-supervised-learning %}), and then fine-tune it on the downstream task with a small set of labeled samples.
2. *Semi-supervised learning*: Learn from the labelled and unlabeled samples together. A lot of research has happened on vision tasks within this approach.
3. *Active learning*: Labeling is expensive, but we still want to collect more given a cost budget. Active learning learns to select most valuable unlabeled samples to be collected next and helps us act smartly with a limited budget.
4. *Pre-training + dataset auto-generation*: Given a capable pre-trained model, we can utilize it to auto-generate a lot more labeled samples. This has been especially popular within the language domain driven by the success of few-shot learning.

I plan to write a series of posts on the topic of “Learning with not enough data”. Part 1 is on *Semi-Supervised Learning*.


{: class="table-of-content"}
* TOC
{:toc}


## What is semi-supervised learning?

Semi-supervised learning uses both labeled and unlabeled data to train a model.

Interestingly most existing literature on semi-supervised learning focuses on vision tasks. And instead pre-training + fine-tuning is a more common paradigm for language tasks.

All the methods introduced in this post have a loss combining two parts: $$\mathcal{L} = \mathcal{L}_s +  \mu(t) \mathcal{L}_u$$. The supervised loss $$\mathcal{L}_s$$ is easy to get given all the labeled examples. We will focus on how the unsupervised loss $$\mathcal{L}_u$$ is designed. A common choice of the weighting term $$\mu(t)$$ is a ramp function increasing the importance of $$\mathcal{L}_u$$ in time, where $$t$$ is the training step.


> *Disclaimer*: The post is not gonna cover semi-supervised methods with focus on model architecture modification. Check [this survey](https://arxiv.org/abs/2006.05278) for how to use generative models and graph-based methods in semi-supervised learning. 


## References

[1] Ouali, Hudelot & Tami. [“An Overview of Deep Semi-Supervised Learning”](https://arxiv.org/abs/2006.05278) arXiv preprint arXiv:2006.05278 (2020).

[2] Sajjadi, Javanmardi & Tasdizen [“Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning.”](https://arxiv.org/abs/1606.04586) arXiv preprint arXiv:1606.04586 (2016).

[3] Pham et al. [“Meta Pseudo Labels.”](https://arxiv.org/abs/2003.10580) CVPR 2021.

[4] Laine & Aila. [“Temporal Ensembling for Semi-Supervised Learning”](https://arxiv.org/abs/1610.02242) ICLR 2017.

[5] Tarvaninen & Valpola. [“Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.”](https://arxiv.org/abs/1703.01780) NeuriPS 2017 

[6] Xie et al. [“Unsupervised Data Augmentation for Consistency Training.”](https://arxiv.org/abs/1904.12848) NeuriPS 2020.

[7] Miyato et al. [“Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning.”](https://arxiv.org/abs/1704.03976) IEEE transactions on pattern analysis and machine intelligence 41.8 (2018).

[8] Verma et al. [“Interpolation consistency training for semi-supervised learning.”](https://arxiv.org/abs/1903.03825) IJCAI 2019 

[9] Lee. [“Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.”](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf) ICML 2013 Workshop: Challenges in Representation Learning.

[10] Iscen et al. [“Label propagation for deep semi-supervised learning.”](https://arxiv.org/abs/1904.04717) CVPR 2019.

[11] Xie et al. [“Self-training with Noisy Student improves ImageNet classification”](https://arxiv.org/abs/1911.04252) CVPR 2020.

[12] Jingfei Du et al. [“Self-training Improves Pre-training for Natural Language Understanding.”](https://arxiv.org/abs/2010.02194) 2020

[13] Iscen et al. [“Label propagation for deep semi-supervised learning.”](https://arxiv.org/abs/1904.04717) CVPR 2019

[14] Arazo et al. [“Pseudo-labeling and confirmation bias in deep semi-supervised learning.”](https://arxiv.org/abs/1908.02983) IJCNN 2020.

[15] Berthelot et al. [“MixMatch: A holistic approach to semi-supervised learning.”](https://arxiv.org/abs/1905.02249) NeuriPS 2019

[16] Berthelot et al. [“ReMixMatch: Semi-supervised learning with distribution alignment and augmentation anchoring.”](https://arxiv.org/abs/1911.09785) ICLR 2020

[17] Sohn et al. [“FixMatch: Simplifying semi-supervised learning with consistency and confidence.”](https://arxiv.org/abs/2001.07685)  CVPR 2020

[18] Junnan Li et al. [“DivideMix: Learning with Noisy Labels as Semi-supervised Learning.”](https://arxiv.org/abs/2002.07394) 2020 [[code](https://github.com/LiJunnan1992/DivideMix)]

[19] Zoph et al. [“Rethinking pre-training and self-training.”](https://arxiv.org/abs/2006.06882) 2020.

[20] Chen et al. [“Big Self-Supervised Models are Strong Semi-Supervised Learners”](https://arxiv.org/abs/2006.10029) 2020